## üß© Beginner / Foundation Scenarios

**1. A pod is stuck in the Pending state. How will you troubleshoot? What are the possible causes?**
   
**Answer-** A pod in Pending means **Kubernetes has accepted the pod, but it has not been scheduled onto any node.**

### üèÜ One-Line Summary (For Interviews)

A pod remains in Pending state when the scheduler cannot find a suitable node due to resource shortages, scheduling constraints, storage issues, or policy restrictions. The fastest way to diagnose is by checking pod events using ```kubectl describe pod```.

### üîç Step-by-Step Troubleshooting Approach
**1Ô∏è‚É£ Check Pod Events**
```bash
kubectl describe pod <pod-name> -n <namespace>
```
üëâ Look at the Events section at the bottom. **This usually tells you exactly why scheduling failed.**

#### ‚ö†Ô∏è Common Causes & How to Identify Them

üîπ **Insufficient Cluster Resources (CPU / Memory)**

**Symptom in events:**
```
0/5 nodes are available: insufficient cpu / insufficient memory
```
#### How to fix:

‚Ä¢	Reduce resource requests \
‚Ä¢	Add more nodes \
‚Ä¢	Enable Cluster Autoscaler

üîπ **Node Selector / Affinity Mismatch**
**Symptom in events:**
```
node(s) didn't match node selector
```

#### Why it happens:

‚Ä¢	Pod has nodeSelector, nodeAffinity, or podAffinity rules \
‚Ä¢	No node matches the required labels

```bash
kubectl get nodes --show-labels
```

**Fix:**

‚Ä¢	Update pod selectors \
‚Ä¢	Add correct labels to nodes

```bash
kubectl label node <node-name> env=prod
```

üîπ **Taints and Tolerations Issue**

**Symptom:**
```
node(s) had taint {key=value:NoSchedule}
```

#### Why it happens:

‚Ä¢	Node is tainted \
‚Ä¢	Pod does not tolerate the taint

**Check taints:**
```bash
kubectl describe node <node-name>
```
**Fix (add toleration):**
```yaml
tolerations:
- key: "dedicated"
  operator: "Equal"
  value: "frontend"
  effect: "NoSchedule"
```

üîπ **PersistentVolumeClaim (PVC) Not Bound**

**Symptom:**
```
pod has unbound immediate PersistentVolumeClaims
```

Why it happens:

‚Ä¢	PVC exists but no matching PV \
‚Ä¢	StorageClass misconfigured

**Check**
```bash
kubectl get pvc
kubectl get pv
```

**Fix:**

‚Ä¢	Create PV \
‚Ä¢	Fix StorageClass \
‚Ä¢	Ensure correct access mode & size

üîπ **Pod Requests Special Resources**

**Examples:**

‚Ä¢	GPUs \
‚Ä¢	HugePages \
‚Ä¢	Custom device plugins

**Symptom:**
```
Insufficient nvidia.com/gpu
```

**Fix:**

‚Ä¢	Add suitable nodes \
‚Ä¢	Reduce special resource requests

üîπ **Namespace ResourceQuota Limit Reached**

**Symptom:**
```
exceeded quota
```

**Check:**
```bash
kubectl describe quota -n <namespace>
```

**Fix:**

‚Ä¢	Increase quota \
‚Ä¢	Reduce pod requests

---

**2. An application pod keeps restarting with CrashLoopBackOff. What steps will you follow to identify and fix the issue?**
**Answer-** The container is repeatedly crashing shortly after starting. Kubernetes attempts to restart the container, but it fails again, creating a loop where the time between restarts increases exponentially (10s, 20s, 40s... up to 5 minutes) to avoid overloading the node. 

### üîç Step-by-Step Troubleshooting Approach

#### 1Ô∏è‚É£ Identify the Exact Failure Reason

**Check pod status & events**

```bash
kubectl describe pod <pod-name> -n <namespace>
```

**Focus on:**
- Last state
- Exit code
- Events

```
Last State: Terminated
Reason: Error
Exit Code: 1
```

#### 2Ô∏è‚É£ Check Application Logs

**Current container logs**

```bash
kubectl logs <pod-name> -n <namespace>
```

**Previous crashed container logs**

```bash
kubectl logs <pod-name> -n <namespace> --previous
```

#### 3Ô∏è‚É£ Identify the Root Cause Category

**üîπ A. Misconfiguration**
- Missing environment variables
- Wrong config values
- App fails to bind to port
- Dependency not reachable (DB, API)

**üîπ B. Wrong Container Command or Entrypoint**

```
exec: "start.sh": no such file or directory
```

**üîπ C. Liveness / Readiness Probe Misconfiguration**

**Fix:**
- Increase initialDelaySeconds
- Fix probe path/port
- Use readiness probe first

**üîπ D. OOMKilled (Memory Issues)**

```
Reason: OOMKilled
Exit Code: 137
```

**Fix:**
- Increase memory limits
- Fix memory leak
- Adjust JVM / runtime memory flags

**üîπ E. Missing Dependencies (ConfigMap / Secret / Volume)**

**Fix:**
- Ensure volume mounts exist
- Correct file paths
- Architecture mismatch (amd64 vs arm64)

---

**3. Pods are running, but the application is not accessible via the Service. What will you check?**
**Answer-** The Service exists, but traffic is not correctly reaching the pods or the application inside the pods.

### üîç Step-by-Step Troubleshooting Flow

#### 1Ô∏è‚É£ Verify Pod Health & Readiness

Even if pods are running, they may not be Ready.

```bash
kubectl get pods -n <namespace>
```

**Check readiness probe:**

```bash
kubectl describe pod <pod-name>
```

**Fix:**
- Correct readiness probe path/port
- Increase initialDelaySeconds

üëâ If the pod is not Ready, Service will NOT send traffic to it.

#### 2Ô∏è‚É£ Check Service Selectors

 ```bash
kubectl describe svc <service-name> -n <namespace>
```

**Verify:**

```yaml
selector:
  app: my-app
```

**Compare with pod labels:**

```bash
kubectl get pods --show-labels
```

‚ùå **If labels don‚Äôt match ‚Üí Service has no endpoints**

#### 3Ô∏è‚É£ Check Service Endpoints

```bash
kubectl get endpoints <service-name> -n <namespace>
```

**Possible outcomes:**
- ‚ùå No endpoints ‚Üí selector mismatch/pod not Ready
- ‚úÖ Endpoints exist ‚Üí move to networking checks

#### 4Ô∏è‚É£ Validate Target Port vs Container Port

**Service:**

```yaml
ports:
- port: 80
  targetPort: 8080
```

**Pod:**

```yaml
containerPort: 8080
```

**‚ùå Mismatch = traffic dropped**

#### 5Ô∏è‚É£ Test Connectivity Inside the Cluster

**Exec into another pod:**

```bash
kubectl exec -it <test-pod> -- curl http://<service-name>:<port>
```

**If this fails:**
- Service misconfiguration
- Network policy blocking traffic

#### 6Ô∏è‚É£ Check Application Binding

**Application must listen on:**

```code
0.0.0.0
```

**‚ùå If app binds to localhost (127.0.0.1) ‚Üí Service cannot reach it**

#### 7Ô∏è‚É£ Check Network Policies
#### 8Ô∏è‚É£ Check kube-proxy / CNI Issues
